{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\360downloads\\\\毕业论文\\\\pytorch'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transformer_ImageNet = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "val_transformer_ImageNet = transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "\n",
    "class MyRemoteDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, transform):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):  # 因为漏了这行代码，花了一个多小时解决问题\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filenames[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "\n",
    "def fetch_dataloaders(data_dir, ratio, batchsize=64):\n",
    "    \"\"\" the sum of ratio must equal to 1\"\"\"\n",
    "    dataset = ImageFolder(data_dir)\n",
    "    character = [[] for i in range(len(dataset.classes))]\n",
    "    for x, y in dataset.samples:  # 将数据按类标存放\n",
    "        character[y].append(x)\n",
    "\n",
    "    train_inputs, val_inputs, test_inputs = [], [], []\n",
    "    train_labels, val_labels, test_labels = [], [], []\n",
    "    for i, data in enumerate(character):\n",
    "        num_sample_train = int(len(data) * ratio[0])\n",
    "        num_sample_val = int(len(data) * ratio[1])\n",
    "        num_val_index = num_sample_train + num_sample_val\n",
    "\n",
    "        for x in data[:num_sample_train]:\n",
    "            train_inputs.append(str(x))\n",
    "            train_labels.append(i)\n",
    "        for x in data[num_sample_train:num_val_index]:\n",
    "            val_inputs.append(str(x))\n",
    "            val_labels.append(i)\n",
    "        for x in data[num_val_index:]:\n",
    "            test_inputs.append(str(x))\n",
    "            test_labels.append(i)\n",
    "\n",
    "    train_dataloader = DataLoader(MyRemoteDataset(train_inputs, train_labels, train_transformer_ImageNet), batch_size=batchsize, drop_last=True, shuffle=True)\n",
    "    val_dataloader = DataLoader(MyRemoteDataset(val_inputs, val_labels, val_transformer_ImageNet), batch_size=batchsize, drop_last=True, shuffle=True)\n",
    "    test_dataloader = DataLoader(MyRemoteDataset(test_inputs, test_labels, val_transformer_ImageNet), batch_size=batchsize, shuffle=False)\n",
    "\n",
    "    loader = {}\n",
    "    loader['train'] = train_dataloader\n",
    "    loader['val'] = val_dataloader\n",
    "    loader['test'] = test_dataloader\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     data_dir = '../数据集/Google dataset of SIRI-WHU_earth_im_tiff/12class_tif'\n",
    "#     # D:\\360downloads\\毕业论文\\数据集\\Google dataset of SIRI-WHU_earth_im_tiff\\12class_tif\n",
    "#     \"\"\" 每一类图片有200张，其中120张用于训练，40张用于测试，40张用于测试\"\"\"\n",
    "#     loader = fetch_dataloaders(data_dir, [0.6, 0.2, 0.2], batchsize=64)\n",
    "#     for x, y in loader['train']:\n",
    "#         x\n",
    "#         y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '../数据集/Google dataset of SIRI-WHU_earth_im_tiff/12class_tif'\n",
    "# # D:\\360downloads\\毕业论文\\数据集\\Google dataset of SIRI-WHU_earth_im_tiff\\12class_tif\n",
    "# #每一类图片有200张，其中120张用于训练，40张用于测试，40张用于测试\n",
    "# loader = fetch_dataloaders(data_dir, [0.6, 0.2, 0.2], batchsize=1)\n",
    "# # for x, y in loader['test']: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
