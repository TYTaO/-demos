{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "0.2.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps \n",
    "print(torch.__version__)  #1.1.0\n",
    "print(torchvision.__version__)  #0.3.0\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle as pk\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "import graduation_pytorch as gra\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "## 创建训练的向量\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:',device)\n",
    "batch_size = 32\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def dataLoader():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # mean = [0.485, 0.456, 0.406]  std = [0.229, 0.224, 0.225]\n",
    "    train_augs = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    ##  !! train\n",
    "    Gallery_dataset = datasets.ImageFolder(root='../数据集/UCMerced_LandUse/data/train', transform=train_augs)\n",
    "    return Gallery_dataset\n",
    "Gallery_dataset = dataLoader()\n",
    "Gallery_loader = DataLoader(Gallery_dataset,\n",
    "                           batch_size = 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 1680\n",
       "    Root Location: ../数据集/UCMerced_LandUse/data/train\n",
       "    Transforms (if any): Compose(\n",
       "                             RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "                             RandomHorizontalFlip(p=0.5)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gallery_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graduation_pytorch.pooling import RMAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载模型：resnet34_whurisi_remote_finetune_\n",
    "pretrained_net = models.resnet34(pretrained=True)\n",
    "pretrained_net.fc = nn.Linear(512, 21)\n",
    "\n",
    "pretrained_net.avgpool = RMAC()\n",
    "PATH = \"./my_model/rmac_L=2_UcRemote_resnet34_fine-tune.pt\" # rmac  滑动窗口\n",
    "\n",
    "pretrained_net.fc = nn.Sequential()\n",
    "pretrained_net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = pretrained_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): RMAC(L=3)\n",
       "  (fc): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用于将图片转化成向量\n",
    "pretrained_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "# Iterate over data.\n",
    "for inputs, labels in Gallery_loader:\n",
    "    i += 1\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = pretrained_net(inputs)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo 一个简单的只涉及全连接层的网络，使用对比损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义Dataset类，__getitem__(self,index)每次返回(img1, img2, 0/1) 返回相同0  或不同类图片1\n",
    "class UcLandSiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs) #37个类别中任选一个       待\n",
    "        should_get_same_class = random.randint(0,1) #保证同类样本约占一半  0: 50%  1: 50%\n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #直到找到同一类别  找到为止\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #直到找到非同一类别\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义文件dataset\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# mean = [0.485, 0.456, 0.406]  std = [0.229, 0.224, 0.225]\n",
    "train_augs = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "training_dir = '../数据集/UCMerced_LandUse/data/train'  #训练集地址\n",
    "folder_dataset = torchvision.datasets.ImageFolder(root=training_dir, transform=train_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义图像dataset\n",
    "siamese_dataset = UcLandSiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
    "                                        transform=train_augs,\n",
    "                                        should_invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义图像dataloader\n",
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "                            shuffle=True,\n",
    "                            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建模型\n",
    "class SiameseNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_inputs, num_outputs, num_hiddens = 512, 512, 1024\n",
    "        # 转向量\n",
    "        self.pre = pretrained_net\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hiddens, num_outputs), \n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.pre(x)\n",
    "#         output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "    \n",
    "    \n",
    "#自定义ContrastiveLoss\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        # 公式\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +  \n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork().cuda() #定义模型且移至GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss() #定义损失函数\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0005) #定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 0 , Current loss: 1.4577\n",
      "\n",
      "Epoch number: 1 , Current loss: 1.3215\n",
      "\n",
      "Epoch number: 2 , Current loss: 1.2067\n",
      "\n",
      "Epoch number: 3 , Current loss: 1.1659\n",
      "\n",
      "Epoch number: 4 , Current loss: 1.1277\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(iteration,loss):\n",
    "    #绘制损失变化图\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()\n",
    "num_epochs = 5\n",
    "\n",
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number = 0\n",
    "\n",
    "#开始训练\n",
    "for epoch in range(0, num_epochs):\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        img0, img1 , label = data\n",
    "        #img0维度为torch.Size([32, 1, 100, 100])，32是batch，label为torch.Size([32, 1])\n",
    "        img0, img1 , label = img0.cuda(), img1.cuda(), label.cuda() #数据移至GPU\n",
    "        optimizer.zero_grad()\n",
    "        output1,output2 = net(img0, img1)\n",
    "        loss_contrastive = criterion(output1, output2, label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0 :\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_contrastive.item())\n",
    "    print(\"Epoch number: {} , Current loss: {:.4f}\\n\".format(epoch,loss_contrastive.item()))\n",
    "    \n",
    "show_plot(counter, loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    img0, img1 , label = data\n",
    "    img0, img1 , label = img0.cuda(), img1.cuda(), label.cuda() #数据移至GPU\n",
    "    output = net.forward_once(img0)\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = outputs.data.cpu()\n",
    "# norm feature\n",
    "fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "ff = ff.div(fnorm.expand_as(ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装距离比较函数\n",
    "def similarity_of_two(img_data1, img_data2):\n",
    "    out = (img_data1*img_data2)\n",
    "    return out.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9553)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_of_two(ff[0], ff[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./my_model/test1_conLoss_rmac_L=2_UcRemote_resnet34_fine-tune.pt\"\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"pre.conv1.weight\", \"pre.bn1.weight\", \"pre.bn1.bias\", \"pre.bn1.running_mean\", \"pre.bn1.running_var\", \"pre.bn1.num_batches_tracked\", \"pre.layer1.0.conv1.weight\", \"pre.layer1.0.bn1.weight\", \"pre.layer1.0.bn1.bias\", \"pre.layer1.0.bn1.running_mean\", \"pre.layer1.0.bn1.running_var\", \"pre.layer1.0.bn1.num_batches_tracked\", \"pre.layer1.0.conv2.weight\", \"pre.layer1.0.bn2.weight\", \"pre.layer1.0.bn2.bias\", \"pre.layer1.0.bn2.running_mean\", \"pre.layer1.0.bn2.running_var\", \"pre.layer1.0.bn2.num_batches_tracked\", \"pre.layer1.1.conv1.weight\", \"pre.layer1.1.bn1.weight\", \"pre.layer1.1.bn1.bias\", \"pre.layer1.1.bn1.running_mean\", \"pre.layer1.1.bn1.running_var\", \"pre.layer1.1.bn1.num_batches_tracked\", \"pre.layer1.1.conv2.weight\", \"pre.layer1.1.bn2.weight\", \"pre.layer1.1.bn2.bias\", \"pre.layer1.1.bn2.running_mean\", \"pre.layer1.1.bn2.running_var\", \"pre.layer1.1.bn2.num_batches_tracked\", \"pre.layer1.2.conv1.weight\", \"pre.layer1.2.bn1.weight\", \"pre.layer1.2.bn1.bias\", \"pre.layer1.2.bn1.running_mean\", \"pre.layer1.2.bn1.running_var\", \"pre.layer1.2.bn1.num_batches_tracked\", \"pre.layer1.2.conv2.weight\", \"pre.layer1.2.bn2.weight\", \"pre.layer1.2.bn2.bias\", \"pre.layer1.2.bn2.running_mean\", \"pre.layer1.2.bn2.running_var\", \"pre.layer1.2.bn2.num_batches_tracked\", \"pre.layer2.0.conv1.weight\", \"pre.layer2.0.bn1.weight\", \"pre.layer2.0.bn1.bias\", \"pre.layer2.0.bn1.running_mean\", \"pre.layer2.0.bn1.running_var\", \"pre.layer2.0.bn1.num_batches_tracked\", \"pre.layer2.0.conv2.weight\", \"pre.layer2.0.bn2.weight\", \"pre.layer2.0.bn2.bias\", \"pre.layer2.0.bn2.running_mean\", \"pre.layer2.0.bn2.running_var\", \"pre.layer2.0.bn2.num_batches_tracked\", \"pre.layer2.0.downsample.0.weight\", \"pre.layer2.0.downsample.1.weight\", \"pre.layer2.0.downsample.1.bias\", \"pre.layer2.0.downsample.1.running_mean\", \"pre.layer2.0.downsample.1.running_var\", \"pre.layer2.0.downsample.1.num_batches_tracked\", \"pre.layer2.1.conv1.weight\", \"pre.layer2.1.bn1.weight\", \"pre.layer2.1.bn1.bias\", \"pre.layer2.1.bn1.running_mean\", \"pre.layer2.1.bn1.running_var\", \"pre.layer2.1.bn1.num_batches_tracked\", \"pre.layer2.1.conv2.weight\", \"pre.layer2.1.bn2.weight\", \"pre.layer2.1.bn2.bias\", \"pre.layer2.1.bn2.running_mean\", \"pre.layer2.1.bn2.running_var\", \"pre.layer2.1.bn2.num_batches_tracked\", \"pre.layer2.2.conv1.weight\", \"pre.layer2.2.bn1.weight\", \"pre.layer2.2.bn1.bias\", \"pre.layer2.2.bn1.running_mean\", \"pre.layer2.2.bn1.running_var\", \"pre.layer2.2.bn1.num_batches_tracked\", \"pre.layer2.2.conv2.weight\", \"pre.layer2.2.bn2.weight\", \"pre.layer2.2.bn2.bias\", \"pre.layer2.2.bn2.running_mean\", \"pre.layer2.2.bn2.running_var\", \"pre.layer2.2.bn2.num_batches_tracked\", \"pre.layer2.3.conv1.weight\", \"pre.layer2.3.bn1.weight\", \"pre.layer2.3.bn1.bias\", \"pre.layer2.3.bn1.running_mean\", \"pre.layer2.3.bn1.running_var\", \"pre.layer2.3.bn1.num_batches_tracked\", \"pre.layer2.3.conv2.weight\", \"pre.layer2.3.bn2.weight\", \"pre.layer2.3.bn2.bias\", \"pre.layer2.3.bn2.running_mean\", \"pre.layer2.3.bn2.running_var\", \"pre.layer2.3.bn2.num_batches_tracked\", \"pre.layer3.0.conv1.weight\", \"pre.layer3.0.bn1.weight\", \"pre.layer3.0.bn1.bias\", \"pre.layer3.0.bn1.running_mean\", \"pre.layer3.0.bn1.running_var\", \"pre.layer3.0.bn1.num_batches_tracked\", \"pre.layer3.0.conv2.weight\", \"pre.layer3.0.bn2.weight\", \"pre.layer3.0.bn2.bias\", \"pre.layer3.0.bn2.running_mean\", \"pre.layer3.0.bn2.running_var\", \"pre.layer3.0.bn2.num_batches_tracked\", \"pre.layer3.0.downsample.0.weight\", \"pre.layer3.0.downsample.1.weight\", \"pre.layer3.0.downsample.1.bias\", \"pre.layer3.0.downsample.1.running_mean\", \"pre.layer3.0.downsample.1.running_var\", \"pre.layer3.0.downsample.1.num_batches_tracked\", \"pre.layer3.1.conv1.weight\", \"pre.layer3.1.bn1.weight\", \"pre.layer3.1.bn1.bias\", \"pre.layer3.1.bn1.running_mean\", \"pre.layer3.1.bn1.running_var\", \"pre.layer3.1.bn1.num_batches_tracked\", \"pre.layer3.1.conv2.weight\", \"pre.layer3.1.bn2.weight\", \"pre.layer3.1.bn2.bias\", \"pre.layer3.1.bn2.running_mean\", \"pre.layer3.1.bn2.running_var\", \"pre.layer3.1.bn2.num_batches_tracked\", \"pre.layer3.2.conv1.weight\", \"pre.layer3.2.bn1.weight\", \"pre.layer3.2.bn1.bias\", \"pre.layer3.2.bn1.running_mean\", \"pre.layer3.2.bn1.running_var\", \"pre.layer3.2.bn1.num_batches_tracked\", \"pre.layer3.2.conv2.weight\", \"pre.layer3.2.bn2.weight\", \"pre.layer3.2.bn2.bias\", \"pre.layer3.2.bn2.running_mean\", \"pre.layer3.2.bn2.running_var\", \"pre.layer3.2.bn2.num_batches_tracked\", \"pre.layer3.3.conv1.weight\", \"pre.layer3.3.bn1.weight\", \"pre.layer3.3.bn1.bias\", \"pre.layer3.3.bn1.running_mean\", \"pre.layer3.3.bn1.running_var\", \"pre.layer3.3.bn1.num_batches_tracked\", \"pre.layer3.3.conv2.weight\", \"pre.layer3.3.bn2.weight\", \"pre.layer3.3.bn2.bias\", \"pre.layer3.3.bn2.running_mean\", \"pre.layer3.3.bn2.running_var\", \"pre.layer3.3.bn2.num_batches_tracked\", \"pre.layer3.4.conv1.weight\", \"pre.layer3.4.bn1.weight\", \"pre.layer3.4.bn1.bias\", \"pre.layer3.4.bn1.running_mean\", \"pre.layer3.4.bn1.running_var\", \"pre.layer3.4.bn1.num_batches_tracked\", \"pre.layer3.4.conv2.weight\", \"pre.layer3.4.bn2.weight\", \"pre.layer3.4.bn2.bias\", \"pre.layer3.4.bn2.running_mean\", \"pre.layer3.4.bn2.running_var\", \"pre.layer3.4.bn2.num_batches_tracked\", \"pre.layer3.5.conv1.weight\", \"pre.layer3.5.bn1.weight\", \"pre.layer3.5.bn1.bias\", \"pre.layer3.5.bn1.running_mean\", \"pre.layer3.5.bn1.running_var\", \"pre.layer3.5.bn1.num_batches_tracked\", \"pre.layer3.5.conv2.weight\", \"pre.layer3.5.bn2.weight\", \"pre.layer3.5.bn2.bias\", \"pre.layer3.5.bn2.running_mean\", \"pre.layer3.5.bn2.running_var\", \"pre.layer3.5.bn2.num_batches_tracked\", \"pre.layer4.0.conv1.weight\", \"pre.layer4.0.bn1.weight\", \"pre.layer4.0.bn1.bias\", \"pre.layer4.0.bn1.running_mean\", \"pre.layer4.0.bn1.running_var\", \"pre.layer4.0.bn1.num_batches_tracked\", \"pre.layer4.0.conv2.weight\", \"pre.layer4.0.bn2.weight\", \"pre.layer4.0.bn2.bias\", \"pre.layer4.0.bn2.running_mean\", \"pre.layer4.0.bn2.running_var\", \"pre.layer4.0.bn2.num_batches_tracked\", \"pre.layer4.0.downsample.0.weight\", \"pre.layer4.0.downsample.1.weight\", \"pre.layer4.0.downsample.1.bias\", \"pre.layer4.0.downsample.1.running_mean\", \"pre.layer4.0.downsample.1.running_var\", \"pre.layer4.0.downsample.1.num_batches_tracked\", \"pre.layer4.1.conv1.weight\", \"pre.layer4.1.bn1.weight\", \"pre.layer4.1.bn1.bias\", \"pre.layer4.1.bn1.running_mean\", \"pre.layer4.1.bn1.running_var\", \"pre.layer4.1.bn1.num_batches_tracked\", \"pre.layer4.1.conv2.weight\", \"pre.layer4.1.bn2.weight\", \"pre.layer4.1.bn2.bias\", \"pre.layer4.1.bn2.running_mean\", \"pre.layer4.1.bn2.running_var\", \"pre.layer4.1.bn2.num_batches_tracked\", \"pre.layer4.2.conv1.weight\", \"pre.layer4.2.bn1.weight\", \"pre.layer4.2.bn1.bias\", \"pre.layer4.2.bn1.running_mean\", \"pre.layer4.2.bn1.running_var\", \"pre.layer4.2.bn1.num_batches_tracked\", \"pre.layer4.2.conv2.weight\", \"pre.layer4.2.bn2.weight\", \"pre.layer4.2.bn2.bias\", \"pre.layer4.2.bn2.running_mean\", \"pre.layer4.2.bn2.running_var\", \"pre.layer4.2.bn2.num_batches_tracked\", \"fc1.0.weight\", \"fc1.0.bias\", \"fc1.2.weight\", \"fc1.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-50e9da6eddb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSiameseNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#定义模型且移至GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./my_model/test1_conLoss_rmac_L=2_UcRemote_resnet34_fine-tune.pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpretrained_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\181818\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 769\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"pre.conv1.weight\", \"pre.bn1.weight\", \"pre.bn1.bias\", \"pre.bn1.running_mean\", \"pre.bn1.running_var\", \"pre.bn1.num_batches_tracked\", \"pre.layer1.0.conv1.weight\", \"pre.layer1.0.bn1.weight\", \"pre.layer1.0.bn1.bias\", \"pre.layer1.0.bn1.running_mean\", \"pre.layer1.0.bn1.running_var\", \"pre.layer1.0.bn1.num_batches_tracked\", \"pre.layer1.0.conv2.weight\", \"pre.layer1.0.bn2.weight\", \"pre.layer1.0.bn2.bias\", \"pre.layer1.0.bn2.running_mean\", \"pre.layer1.0.bn2.running_var\", \"pre.layer1.0.bn2.num_batches_tracked\", \"pre.layer1.1.conv1.weight\", \"pre.layer1.1.bn1.weight\", \"pre.layer1.1.bn1.bias\", \"pre.layer1.1.bn1.running_mean\", \"pre.layer1.1.bn1.running_var\", \"pre.layer1.1.bn1.num_batches_tracked\", \"pre.layer1.1.conv2.weight\", \"pre.layer1.1.bn2.weight\", \"pre.layer1.1.bn2.bias\", \"pre.layer1.1.bn2.running_mean\", \"pre.layer1.1.bn2.running_var\", \"pre.layer1.1.bn2.num_batches_tracked\", \"pre.layer1.2.conv1.weight\", \"pre.layer1.2.bn1.weight\", \"pre.layer1.2.bn1.bias\", \"pre.layer1.2.bn1.running_mean\", \"pre.layer1.2.bn1.running_var\", \"pre.layer1.2.bn1.num_batches_tracked\", \"pre.layer1.2.conv2.weight\", \"pre.layer1.2.bn2.weight\", \"pre.layer1.2.bn2.bias\", \"pre.layer1.2.bn2.running_mean\", \"pre.layer1.2.bn2.running_var\", \"pre.layer1.2.bn2.num_batches_tracked\", \"pre.layer2.0.conv1.weight\", \"pre.layer2.0.bn1.weight\", \"pre.layer2.0.bn1.bias\", \"pre.layer2.0.bn1.running_mean\", \"pre.layer2.0.bn1.running_var\", \"pre.layer2.0.bn1.num_batches_tracked\", \"pre.layer2.0.conv2.weight\", \"pre.layer2.0.bn2.weight\", \"pre.layer2.0.bn2.bias\", \"pre.layer2.0.bn2.running_mean\", \"pre.layer2.0.bn2.running_var\", \"pre.layer2.0.bn2.num_batches_tracked\", \"pre.layer2.0.downsample.0.weight\", \"pre.layer2.0.downsample.1.weight\", \"pre.layer2.0.downsample.1.bias\", \"pre.layer2.0.downsample.1.running_mean\", \"pre.layer2.0.downsample.1.running_var\", \"pre.layer2.0.downsample.1.num_batches_tracked\", \"pre.layer2.1.conv1.weight\", \"pre.layer2.1.bn1.weight\", \"pre.layer2.1.bn1.bias\", \"pre.layer2.1.bn1.running_mean\", \"pre.layer2.1.bn1.running_var\", \"pre.layer2.1.bn1.num_batches_tracked\", \"pre.layer2.1.conv2.weight\", \"pre.layer2.1.bn2.weight\", \"pre.layer2.1.bn2.bias\", \"pre.layer2.1.bn2.running_mean\", \"pre.layer2.1.bn2.running_var\", \"pre.layer2.1.bn2.num_batches_tracked\", \"pre.layer2.2.conv1.weight\", \"pre.layer2.2.bn1.weight\", \"pre.layer2.2.bn1.bias\", \"pre.layer2.2.bn1.running_mean\", \"pre.layer2.2.bn1.running_var\", \"pre.layer2.2.bn1.num_batches_tracked\", \"pre.layer2.2.conv2.weight\", \"pre.layer2.2.bn2.weight\", \"pre.layer2.2.bn2.bias\", \"pre.layer2.2.bn2.running_mean\", \"pre.layer2.2.bn2.running_var\", \"pre.layer2.2.bn2.num_batches_tracked\", \"pre.layer2.3.conv1.weight\", \"pre.layer2.3.bn1.weight\", \"pre.layer2.3.bn1.bias\", \"pre.layer2.3.bn1.running_mean\", \"pre.layer2.3.bn1.running_var\", \"pre.layer2.3.bn1.num_batches_tracked\", \"pre.layer2.3.conv2.weight\", \"pre.layer2.3.bn2.weight\", \"pre.layer2.3.bn2.bias\", \"pre.layer2.3.bn2.running_mean\", \"pre.layer2.3.bn2.running_var\", \"pre.layer2.3.bn2.num_batches_tracked\", \"pre.layer3.0.conv1.weight\", \"pre.layer3.0.bn1.weight\", \"pre.layer3.0.bn1.bias\", \"pre.layer3.0.bn1.running_mean\", \"pre.layer3.0.bn1.running_var\", \"pre.layer3.0.bn1.num_batches_tracked\", \"pre.layer3.0.conv2.weight\", \"pre.layer3.0.bn2.weight\", \"pre.layer3.0.bn2.bias\", \"pre.layer3.0.bn2.running_mean\", \"pre.layer3.0.bn2.running_var\", \"pre.layer3.0.bn2.num_batches_tracked\", \"pre.layer3.0.downsample.0.weight\", \"pre.layer3.0.downsample.1.weight\", \"pre.layer3.0.downsample.1.bias\", \"pre.layer3.0.downsample.1.running_mean\", \"pre.layer3.0.downsample.1.running_var\", \"pre.layer3.0.downsample.1.num_batches_tracked\", \"pre.layer3.1.conv1.weight\", \"pre.layer3.1.bn1.weight\", \"pre.layer3.1.bn1.bias\", \"pre.layer3.1.bn1.running_mean\", \"pre.layer3.1.bn1.running_var\", \"pre.layer3.1.bn1.num_batches_tracked\", \"pre.layer3.1.conv2.weight\", \"pre.layer3.1.bn2.weight\", \"pre.layer3.1.bn2.bias\", \"pre.layer3.1.bn2.running_mean\", \"pre.layer3.1.bn2.running_var\", \"pre.layer3.1.bn2.num_batches_tracked\", \"pre.layer3.2.conv1.weight\", \"pre.layer3.2.bn1.weight\", \"pre.layer3.2.bn1.bias\", \"pre.layer3.2.bn1.running_mean\", \"pre.layer3.2.bn1.running_var\", \"pre.layer3.2.bn1.num_batches_tracked\", \"pre.layer3.2.conv2.weight\", \"pre.layer3.2.bn2.weight\", \"pre.layer3.2.bn2.bias\", \"pre.layer3.2.bn2.running_mean\", \"pre.layer3.2.bn2.running_var\", \"pre.layer3.2.bn2.num_batches_tracked\", \"pre.layer3.3.conv1.weight\", \"pre.layer3.3.bn1.weight\", \"pre.layer3.3.bn1.bias\", \"pre.layer3.3.bn1.running_mean\", \"pre.layer3.3.bn1.running_var\", \"pre.layer3.3.bn1.num_batches_tracked\", \"pre.layer3.3.conv2.weight\", \"pre.layer3.3.bn2.weight\", \"pre.layer3.3.bn2.bias\", \"pre.layer3.3.bn2.running_mean\", \"pre.layer3.3.bn2.running_var\", \"pre.layer3.3.bn2.num_batches_tracked\", \"pre.layer3.4.conv1.weight\", \"pre.layer3.4.bn1.weight\", \"pre.layer3.4.bn1.bias\", \"pre.layer3.4.bn1.running_mean\", \"pre.layer3.4.bn1.running_var\", \"pre.layer3.4.bn1.num_batches_tracked\", \"pre.layer3.4.conv2.weight\", \"pre.layer3.4.bn2.weight\", \"pre.layer3.4.bn2.bias\", \"pre.layer3.4.bn2.running_mean\", \"pre.layer3.4.bn2.running_var\", \"pre.layer3.4.bn2.num_batches_tracked\", \"pre.layer3.5.conv1.weight\", \"pre.layer3.5.bn1.weight\", \"pre.layer3.5.bn1.bias\", \"pre.layer3.5.bn1.running_mean\", \"pre.layer3.5.bn1.running_var\", \"pre.layer3.5.bn1.num_batches_tracked\", \"pre.layer3.5.conv2.weight\", \"pre.layer3.5.bn2.weight\", \"pre.layer3.5.bn2.bias\", \"pre.layer3.5.bn2.running_mean\", \"pre.layer3.5.bn2.running_var\", \"pre.layer3.5.bn2.num_batches_tracked\", \"pre.layer4.0.conv1.weight\", \"pre.layer4.0.bn1.weight\", \"pre.layer4.0.bn1.bias\", \"pre.layer4.0.bn1.running_mean\", \"pre.layer4.0.bn1.running_var\", \"pre.layer4.0.bn1.num_batches_tracked\", \"pre.layer4.0.conv2.weight\", \"pre.layer4.0.bn2.weight\", \"pre.layer4.0.bn2.bias\", \"pre.layer4.0.bn2.running_mean\", \"pre.layer4.0.bn2.running_var\", \"pre.layer4.0.bn2.num_batches_tracked\", \"pre.layer4.0.downsample.0.weight\", \"pre.layer4.0.downsample.1.weight\", \"pre.layer4.0.downsample.1.bias\", \"pre.layer4.0.downsample.1.running_mean\", \"pre.layer4.0.downsample.1.running_var\", \"pre.layer4.0.downsample.1.num_batches_tracked\", \"pre.layer4.1.conv1.weight\", \"pre.layer4.1.bn1.weight\", \"pre.layer4.1.bn1.bias\", \"pre.layer4.1.bn1.running_mean\", \"pre.layer4.1.bn1.running_var\", \"pre.layer4.1.bn1.num_batches_tracked\", \"pre.layer4.1.conv2.weight\", \"pre.layer4.1.bn2.weight\", \"pre.layer4.1.bn2.bias\", \"pre.layer4.1.bn2.running_mean\", \"pre.layer4.1.bn2.running_var\", \"pre.layer4.1.bn2.num_batches_tracked\", \"pre.layer4.2.conv1.weight\", \"pre.layer4.2.bn1.weight\", \"pre.layer4.2.bn1.bias\", \"pre.layer4.2.bn1.running_mean\", \"pre.layer4.2.bn1.running_var\", \"pre.layer4.2.bn1.num_batches_tracked\", \"pre.layer4.2.conv2.weight\", \"pre.layer4.2.bn2.weight\", \"pre.layer4.2.bn2.bias\", \"pre.layer4.2.bn2.running_mean\", \"pre.layer4.2.bn2.running_var\", \"pre.layer4.2.bn2.num_batches_tracked\", \"fc1.0.weight\", \"fc1.0.bias\", \"fc1.2.weight\", \"fc1.2.bias\". "
     ]
    }
   ],
   "source": [
    "net = SiameseNetwork() #定义模型且移至GPU\n",
    "PATH = \"./my_model/test1_conLoss_rmac_L=2_UcRemote_resnet34_fine-tune.pt\"\n",
    "pretrained_net.load_state_dict(torch.load(PATH))\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def dataLoader():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # mean = [0.485, 0.456, 0.406]  std = [0.229, 0.224, 0.225]\n",
    "    test_augs = transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    Gallery_dataset = datasets.ImageFolder(root='../数据集/UCMerced_LandUse/data/test', transform=test_augs)\n",
    "#     Gallery_dataset = datasets.ImageFolder(root='../数据集/Google dataset of SIRI-WHU_earth_im_tiff/12class_tif', transform=test_augs)\n",
    "    return Gallery_dataset\n",
    "Gallery_dataset = dataLoader()\n",
    "Gallery_loader = DataLoader(Gallery_dataset,\n",
    "                           batch_size = 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 6.00 GiB total capacity; 4.50 GiB already allocated; 57.01 MiB free; 65.00 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-aefcd49e89be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Load features.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGallery_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Save features to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-aefcd49e89be>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# norm feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-ae291891df87>\u001b[0m in \u001b[0;36mforward_once\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m#         output = output.view(output.size()[0], -1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\181818\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\181818\\Anaconda\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\181818\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\181818\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\181818\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1623\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1624\u001b[0m     )\n\u001b[0;32m   1625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 6.00 GiB total capacity; 4.50 GiB already allocated; 57.01 MiB free; 65.00 MiB cached)"
     ]
    }
   ],
   "source": [
    "# save to loacl file\n",
    "# save_path = './features/maxPooling_UcRemote21-train-resnet'\n",
    "# save_path = './features/gem_p=3_UcRemote21-train-resnet'\n",
    "save_path = './features/test1_conLoss_rmac_L=2_UcRemote21-train-resnet'\n",
    "\n",
    "def save_data(file, data):\n",
    "    f = open(file, 'wb+')\n",
    "    pk.dump(data, f, 0)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def extract_features(model, loader):\n",
    "    since = time.time()\n",
    "    features = torch.FloatTensor()\n",
    "         \n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model.forward_once(inputs)\n",
    "        ff = outputs.data.cpu()\n",
    "        # norm feature\n",
    "        fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "        ff = ff.div(fnorm.expand_as(ff))\n",
    "        features = torch.cat((features, ff), 0)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Feature extraction complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return features\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    fo = open(save_path, 'rb')\n",
    "    features = pk.load(fo, encoding='bytes')\n",
    "    fo.close()\n",
    "    print(\"Load features.\")\n",
    "else:\n",
    "    features = extract_features(net, Gallery_loader)\n",
    "    save_data(save_path, features)\n",
    "    print(\"Save features to %s\" % (save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
